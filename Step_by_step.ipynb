{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" width=\"400\">\n",
    "\n",
    "<h1><center>  Data Challenge X-PSG  </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, i'll try to explain my approach of the X-PSG Data Challenge. Let's look into it step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Outline\n",
    "\n",
    "## <a href=\"#1.-Load-Data \" style=\"color:#00925B\">1.&nbsp;&nbsp;Load Data</a>\n",
    "\n",
    "## <a href=\"#2.-Data-Processing \" style=\"color:#00925B\">2.&nbsp;&nbsp;Data Processing</a>\n",
    "\n",
    "- <a href=\"#2.1.-Get-Players-that-played-more-than-800-min\" style=\"color:#00925B\">2.1.&nbsp;&nbsp;Get Players that played more than 800 min</a>\n",
    "\n",
    "- <a href=\"#2.2.-Get-Teams-with-Players_ID\" style=\"color:#00925B\">2.2.&nbsp;&nbsp;Get Teams with Players_ID</a>\n",
    "\n",
    "-  <a href=\"#2.3.-Team-Trick\" style=\"color:#00925B\">2.3.&nbsp;&nbsp;Team Trick</a>\n",
    "\n",
    "## <a href=\"#3.-Features-Selection\" style=\"color:#00925B\">3.&nbsp;&nbsp;Feature Selection</a>\n",
    "\n",
    "-  <a href=\"#3.1.-Selection-of-features\" style=\"color:#00925B\">3.1.&nbsp;&nbsp;Feature Selection</a>\n",
    "\n",
    "-  <a href=\"#3.2.-Extract-feature-vectors\" style=\"color:#00925B\">3.2.&nbsp;&nbsp;Extract-feature-vectors</a>\n",
    "\n",
    "-  <a href=\"#3.3.-Load-Train-and-Validation-Data\" style=\"color:#00925B\">3.3.&nbsp;&nbsp; Load Train and Validation Data</a>\n",
    "\n",
    "## <a href=\"#4.-Player-Prediction-Model\" style=\"color:#00925B\">4.&nbsp;&nbsp;Player Prediction Model</a>\n",
    "\n",
    "-  <a href=\"#4.1.-Random-Forest\" style=\"color:#00925B\">4.1.&nbsp;&nbsp;Random Forest</a>\n",
    "-  <a href=\"#4.2.-Neural-Nets\" style=\"color:#00925B\">4.2.&nbsp;&nbsp;Neural Nets</a>\n",
    "\n",
    "## <a href=\"#5.-Predict-Next-Team\" style=\"color:#00925B\">5.&nbsp;&nbsp;Predict Next Team</a>\n",
    "-  <a href=\"#5.1.-Model-n°1\" style=\"color:#00925B\">5.1.&nbsp;&nbsp;Model n°1</a>\n",
    "-  <a href=\"#5.2.-Model-n°2\" style=\"color:#00925B\">5.2.&nbsp;&nbsp;Model n°2</a>\n",
    "\n",
    "## <a href=\"#6.-Predict-Next-Coordinate\" style=\"color:#00925B\">6.&nbsp;&nbsp;Predict Next Coordinate</a>\n",
    "\n",
    "## <a href=\"#7.-Comparaison-with-or-without-team-trick\" style=\"color:#00925B\">7.&nbsp;&nbsp;Bonus : Team Trick</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jyquickhelper import add_notebook_menu\n",
    "# add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages to parse the xml files\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "import xml.etree.ElementTree as et \n",
    "\n",
    "# Packages to deal with dataframes, and numpy array and general processing\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Uselful packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "# Keras models and objets\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, \\\n",
    "                         RepeatVector, Concatenate, Activation, Flatten,\\\n",
    "                         Input, BatchNormalization, Dropout, concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to parse the data from the xml files at our disposal. To do so, we'll create three dataframe that deals respectively with the general game information, the event occuring during the game, and the qualifier features related to those event. \n",
    "\n",
    "To have a general idea the amount of information :\n",
    "- the game related dataframe is 1 line of 10 features\n",
    "- the event related dataframe is around 1812 rows of 16 features\n",
    "- the qualifier related dataframe is around 200 000 rows of 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse XML file to get three dataframe : \n",
    "- game_df : info in the game\n",
    "- even_df : rows on the differents events that occur during the game\n",
    "- q_df : qualifiers table with information regarding events (several qualifiers can describe the same event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load load_data.py\n",
    "from load_data import parse_xml_file, event_df_columns, game_df_columns, q_df_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = r\"Exemple de fichier pour base de test - f24-24-2016-853285-eventdetails_test_hackathon_2.xml\"\n",
    "#test_file = r'D:\\PSG\\French Ligue One 20162017 season - Match Day 1- 19\\f24-24-2016-853139-eventdetails.xml'\n",
    "game_df, event_df, q_df = parse_xml_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_event</th>\n",
       "      <th>id</th>\n",
       "      <th>qualifier_id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>213</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>107</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>212</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_event id qualifier_id value\n",
       "0         0               1      \n",
       "1         0             213   2.9\n",
       "2         0                      \n",
       "3         0             107      \n",
       "4         0             212  31.7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_game</th>\n",
       "      <th>own_id</th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>period_id</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>player_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>outcome</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>646</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>647</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>648</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>649</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>650</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_game  own_id id event_id type_id period_id min sec player_id team_id  \\\n",
       "0               0         646       1         2  63   0         0       0   \n",
       "1               1         647       1         2  63   4         0       0   \n",
       "2               2         648       1         2  63   7         0       0   \n",
       "3               3         649       1         2  63  10         1       0   \n",
       "4               4         650       1         2  63  13         0       0   \n",
       "\n",
       "  outcome    x    y timestamp last_modified version  \n",
       "0       1  0.0  0.0                                  \n",
       "1       1  0.0  0.0                                  \n",
       "2       1  0.0  0.0                                  \n",
       "3       1  0.0  0.0                                  \n",
       "4       1  0.0  0.0                                  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Get Players that played more than 800 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second important thing we need to do is to get the list of player that played at least 800 minutes during the first part of the season. \n",
    "\n",
    "To do so we create a dictionnary of the cumulated amount of minutes of each player that played a game during at least one game. Then we keep only players that have a cumulated time above 800. Remark: to be sure that we cover the range of player, we'll threshold at 780 min (it only add 4 player regarding to 800 min so it is not a big deal). The safer, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ID of players that played more than 800 min in the first part of the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load active_players.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 active players that played more than 800 minutes\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open('time_per_player.pickle', 'rb') as handle:\n",
    "    dictionnary_time_per_player = pickle.load(handle)\n",
    "    \n",
    "import pickle \n",
    "with open('player_names.pickle', 'rb') as handle:\n",
    "    player_names = pickle.load(handle)\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(dictionnary_time_per_player)\n",
    "c.most_common(10)\n",
    "\n",
    "active_player_800 = [player_id for player_id, value in dictionnary_time_per_player.items() if value >= 780]\n",
    "print('{} active players that played more than 800 minutes'.format(len(active_player_800)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len([player_id for player_id, value in dictionnary_time_per_player.items() if value >= 780])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have two complementary dictionnary to map players with their unique ID we'll give them between 0 and 230. This will be necessary to get the ID out of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_to_idx = {}\n",
    "for i,j in enumerate(active_player_800):\n",
    "    player_to_idx[str(j)] = i\n",
    "#player_to_idx\n",
    "\n",
    "idx_to_player = {}\n",
    "for i,j in enumerate(active_player_800):\n",
    "    idx_to_player[i] = str(j)\n",
    "#player_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['171101', '37832', '168568', '59963', '38816', '44488', '169007', '59957', '54772', '149519']\n",
      "- Unknown player\n",
      "- Jérémy Morel\n",
      "- Corentin Tolisso\n",
      "- Maxime Gonalons\n",
      "- Guy NDy Assembe\n"
     ]
    }
   ],
   "source": [
    "print(active_player_800[:10])\n",
    "for player in active_player_800[:5]:\n",
    "    print('-', player_names.get(player, 'Unknown player'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Get Teams with Players_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In can be interesting to get the list of players and more specificly active player we might predict in each team. This might be potentially helpful if we choose to predict the team first and then the players for instance.\n",
    "\n",
    "To do so, we'll look at the qualifier_id 30 that give the squad of players both on the pitch and on the bench. This will give us the player involved during the first part of the season. Remark : we'll only look at games after September, in case the player has been transfered in another club in August for instance, as it happended with M'Bappé two years ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_games = r'D:\\PSG\\French Ligue One 20162017 season - Match Day 1- 19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b185f663a8cd481e81990bf96d37537e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dic_team = {}\n",
    "team_name = {}\n",
    "all_games = sorted(os.listdir(folder_games))\n",
    "\n",
    "for idx in tqdm_notebook(range(len(all_games))):\n",
    "    file =  os.path.join(folder_games,all_games[idx])\n",
    "    game_df,event_df,q_df =  parse_xml_file(file)\n",
    "    \n",
    "    if game_df.loc[game_df.index[0],'game_date'][5:7] < '09':\n",
    "        continue\n",
    "        \n",
    "    away_team =  game_df.loc[game_df.index[0],'away_team_name']\n",
    "    away_id = int(game_df.loc[game_df.index[0],'away_team_id'])\n",
    "    \n",
    "    home_team =  game_df.loc[game_df.index[0],'home_team_name']\n",
    "    home_id = int(game_df.loc[game_df.index[0],'home_team_id'])\n",
    "\n",
    "    if team_name.get(home_id,'') != '':\n",
    "        assert team_name[home_id] == home_team\n",
    "        \n",
    "    if team_name.get(away_id,'') != '':\n",
    "        try :\n",
    "            assert team_name[away_id] == away_team\n",
    "        except : \n",
    "            print(team_name[away_id], away_team)\n",
    "            \n",
    "    team_name[home_id] = home_team\n",
    "    team_name[away_id] = away_team\n",
    "\n",
    "    q_df = q_df[q_df['qualifier_id']=='30']\n",
    "\n",
    "    for i in q_df.index[:2]:\n",
    "        event_id = q_df.loc[i, 'id_event']\n",
    "        player_list =  q_df.loc[i, 'value'].split(', ')\n",
    "        team_id = int(event_df[event_df['own_id'] == event_id].team_id)\n",
    "        dic_team[team_id] = dic_team.get(team_id, []) + player_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of players involved in a Ligue 1 game for each team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nancy 26\n",
      "Lorient 29\n",
      "Lille 29\n",
      "Monaco 24\n",
      "Marseille 24\n",
      "Nice 25\n",
      "Paris Saint-Germain 26\n",
      "St Etienne 31\n",
      "Toulouse 28\n",
      "Bastia 28\n",
      "Dijon 24\n",
      "Angers 26\n",
      "Caen 24\n",
      "Rennes 30\n",
      "Nantes 24\n",
      "Metz 26\n",
      "Guingamp 25\n",
      "Montpellier 26\n",
      "Lyon 27\n",
      "Bordeaux 28\n"
     ]
    }
   ],
   "source": [
    "tot = []\n",
    "for team in dic_team.keys():\n",
    "    #if team == 146:\n",
    "    #    print(Counter( dic_team[team]).most_common(10))\n",
    "    tot += dic_team[team]\n",
    "    dic_team[team] = set(dic_team[team])\n",
    "    print(team_name[team],len(list(dic_team[team])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save lots of small dictionnary like those two, it will be easier for submitting the prediction files (one will just have to load pickle files and use them directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('team_names.pkl', 'wb') as handle:\n",
    "    pickle.dump(team_name, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('team_squad.pkl', 'wb') as handle:\n",
    "    pickle.dump(dic_team, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exemple, we can look at active players that played more than 800 minutes and that we might want to to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEAM : Paris Saint-Germain\n",
      "116406 Marcos Aoas Correa  -------->\n",
      "51090 Thiago Emiliano da Silva  -------->\n",
      "95715 Lucas Rodrigues Moura da Silva  -------->\n",
      "93605 Thomas Meunier\n",
      "80226 Serge Aurier  -------->\n",
      "54782 Javier Pastore\n",
      "11527 Thiago Motta  -------->\n",
      "225897 No match\n",
      "37971 Blaise Matuidi  -------->\n",
      "40720 Edinson Cavani  -------->\n",
      "93127 Jesé Rodríguez Ruiz\n",
      "120743 Adrien Rabiot  -------->\n",
      "198523 Jean-Kévin Augustin\n",
      "52482 Kevin Trapp\n",
      "129479 Presnel Kimpembe  -------->\n",
      "46497 Ángel Di María  -------->\n",
      "9809 Maxwell Scherrer Cabelino Andrade\n",
      "213199 Remy Descamps\n",
      "213198 Christopher Nkunku\n",
      "199672 Nanitamo Jonathan Ikone\n",
      "212950 Lorenzo Callegari\n",
      "89068 Layvin Kurzawa  -------->\n",
      "73494 Grzegorz Krychowiak\n",
      "18753 Hatem Ben Arfa\n",
      "84182 Alphonse Areola  -------->\n",
      "61170 Marco Verratti  -------->\n"
     ]
    }
   ],
   "source": [
    "print('TEAM :',team_name[149])\n",
    "for player in list(dic_team[149]):\n",
    "    if player in active_player_800 :\n",
    "        print(player, player_names.get(player,'No match'), ' -------->')\n",
    "    else : \n",
    "        print(player, player_names.get(player,'No match'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/PSG_active_players.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the dictionnary of the active players per team (usually between 9 and 13 per team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_team_active = {}\n",
    "for team in dic_team.keys():\n",
    "    l=[]\n",
    "    for player in list(dic_team[team]):\n",
    "        if player in active_player_800 :\n",
    "            l.append(player)\n",
    "    dic_team_active[team] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nancy (148) 9 active players\n",
      "Lorient (694) 11 active players\n",
      "Lille (429) 11 active players\n",
      "Monaco (146) 10 active players\n",
      "Marseille (144) 12 active players\n",
      "Nice (1395) 12 active players\n",
      "Paris Saint-Germain (149) 13 active players\n",
      "St Etienne (152) 12 active players\n",
      "Toulouse (427) 13 active players\n",
      "Bastia (139) 13 active players\n",
      "Dijon (2130) 13 active players\n",
      "Angers (2128) 11 active players\n",
      "Caen (1028) 11 active players\n",
      "Rennes (150) 12 active players\n",
      "Nantes (430) 11 active players\n",
      "Metz (145) 11 active players\n",
      "Guingamp (428) 10 active players\n",
      "Montpellier (147) 10 active players\n",
      "Lyon (143) 13 active players\n",
      "Bordeaux (140) 12 active players\n"
     ]
    }
   ],
   "source": [
    "with open('team_squad_active.pickle', 'wb') as handle:\n",
    "    pickle.dump(dic_team_active, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "for team  in dic_team.keys():\n",
    "    print(team_name[team],'({})'.format(team), len(dic_team_active[team]), 'active players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_team, team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map as well teams with a unique ID between 0 and 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 0 Bastia\n",
      "140 1 Bordeaux\n",
      "143 2 Lyon\n",
      "144 3 Marseille\n",
      "145 4 Metz\n",
      "146 5 Monaco\n",
      "147 6 Montpellier\n",
      "148 7 Nancy\n",
      "149 8 Paris Saint-Germain\n",
      "150 9 Rennes\n",
      "152 10 St Etienne\n",
      "427 11 Toulouse\n",
      "428 12 Guingamp\n",
      "429 13 Lille\n",
      "430 14 Nantes\n",
      "694 15 Lorient\n",
      "1028 16 Caen\n",
      "1395 17 Nice\n",
      "2128 18 Angers\n",
      "2130 19 Dijon\n"
     ]
    }
   ],
   "source": [
    "list_team = sorted(dic_team.keys())\n",
    "\n",
    "team_idx = {}\n",
    "for i,j in enumerate(list_team):\n",
    "    team_idx[j] = i \n",
    "    \n",
    "for key in team_idx.keys():\n",
    "    print(key,team_idx[key], team_name[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Team Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First task, we might want to predict the team playing. \n",
    "\n",
    "By looking into the test file (after procedure), we can see that some information has been left out of the cleaning and it concerns the value of qualifier ID related to the composition of team during substitution or change of captain. By looking at qualifier id 7, 30, 53, 194, 281, one can recover the players involved in the game and figure out very quickly which team is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2130, 7)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From a set of player involves during a game, we get the best match of potential team playing\n",
    "def get_team(set_player):\n",
    "    best_match, inter = '', 0\n",
    "    for team in dic_team.keys():\n",
    "        inter_list = dic_team[team].intersection(set_player)\n",
    "        if len(inter_list) > inter :\n",
    "            inter = len(inter_list)\n",
    "            best_match = team\n",
    "    return best_match, inter\n",
    "\n",
    "print(get_team(set('60582, 102747, 15930, 116643, 71403, 178478, 167449, \\\n",
    "                169102, 48847, 19624, 165809, 7446, 198065, 172987, 220160, 192292'.split(', '))))\n",
    "\n",
    "get_team(set('93619, 100955, 110347, 106263, 44312, 121111, 86158,\\\n",
    "50148, 147577'.split(', ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we look at the test file, one can find out that it was the game Monaco-Dijon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team  1 must be  2130\n",
      "{'111265', '50148', '119679', '98746', '54413', '153014', '13939', '212325', '121111', '93619', '106263', '100955', '147577'}\n",
      "{'111265', '50148', '54413', '153014', '121111', '93619', '106263', '100955', '147577'}\n",
      "Team  0 must be  146\n",
      "{'116643', '165809', '97041', '71403', '102826', '102747', '169102', '60582', '167449', '162654'}\n",
      "{'116643', '165809', '71403', '102747', '169102', '60582', '167449'}\n"
     ]
    }
   ],
   "source": [
    "test_file = r\"Exemple de fichier pour base de test - f24-24-2016-853285-eventdetails_test_hackathon_2.xml\"\n",
    "\n",
    "game_df_test, event_df_test, q_df = parse_xml_file(test_file)\n",
    "q_id_with_players = [7, 30 , 53, 194, 281]\n",
    "teams= {}\n",
    "\n",
    "for i in q_id_with_players:\n",
    "    #if i == 30:\n",
    "    #    print(q_df[q_df['qualifier_id']==str(i)].id_event.unique())\n",
    "    df_chunk =  q_df[q_df['qualifier_id']==str(i)] \n",
    "    for row in df_chunk.iterrows():\n",
    "        id_event = row[1]['id_event']\n",
    "        value = row[1]['value'].split(', ')\n",
    "        #print(id_event,value)\n",
    "        id_team = event_df_test[event_df_test['own_id'] ==  id_event].team_id.values[0]\n",
    "        teams[id_team] = teams.get(id_team,[]) + value\n",
    "        \n",
    "for t in teams : \n",
    "    teams[t] = set(teams[t])\n",
    "    team, val = get_team(teams[t])\n",
    "    if val > 1 :\n",
    "        print('Team ',t, 'must be ',team)\n",
    "        print(dic_team[team].intersection(active_player_800))\n",
    "        print(teams[t].intersection(active_player_800))\n",
    "\n",
    "    #event_df_test[event_df_test['event_id']=='700']\n",
    "    #print(q_df[q_df['qualifier_id']==str(i)].id_event.unique(), q_df[q_df['qualifier_id']==str(i)].value.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can get, if available, the team playing and more specifically the player's team we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_id(xml_file):\n",
    "    \n",
    "    game_df, event_df, q_df = parse_xml_file(xml_file)\n",
    "    \n",
    "    q_id_with_players = [7, 30 , 53, 194, 281]\n",
    "    teams= {}\n",
    "\n",
    "    for i in q_id_with_players:\n",
    "        df_chunk =  q_df[q_df['qualifier_id']==str(i)] \n",
    "        for row in df_chunk.iterrows():\n",
    "            id_event = row[1]['id_event']\n",
    "            value = row[1]['value'].split(', ')\n",
    "            #print(id_event,value)\n",
    "            id_team = event_df_test[event_df_test['own_id'] ==  id_event].team_id.values[0]\n",
    "            teams[id_team] = teams.get(id_team,[]) + value\n",
    "    \n",
    "    team_dic = {}\n",
    "    for t in range(2) : \n",
    "        teams[t] = set(teams[str(t)])\n",
    "        team, val = get_team(teams[t])\n",
    "        if val > 1 :\n",
    "            team_dic[str(t)] = team\n",
    "        #print(team_name[team])\n",
    "    return team_dic\n",
    "\n",
    "def player_team_id(xml_file):\n",
    "    team_playing = get_team_id(test_file)\n",
    "    \n",
    "    game_df, event_df, q_df = parse_xml_file(xml_file)\n",
    "    team_id = event_df[event_df['player_id'] == '1'].team_id.unique()[0]\n",
    "    return team_playing.get(team_id, 'None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, here, we can guess it is Dijon (home) - Monaco (away) and that the player of interest if playing at Monaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home team :  Dijon \n",
      "Away team :  Monaco\n"
     ]
    }
   ],
   "source": [
    "test_file = r\"Exemple de fichier pour base de test - f24-24-2016-853285-eventdetails_test_hackathon_2.xml\"\n",
    "team_playing = get_team_id(test_file)\n",
    "is_away = team_playing.get('0', 'None')\n",
    "is_home = team_playing.get('1', 'None')\n",
    "print('Home team : ', team_name[is_home], '\\nAway team : ', team_name[is_away] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 Monaco\n"
     ]
    }
   ],
   "source": [
    "player_team = player_team_id(test_file)\n",
    "print(player_team, team_name[player_team])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main useful application of this trick is that we can get a restricted list of player that might correspond to the player of interest. Instead of looking at the whole list of 230 players, here we can restrict our prediction to only 9-10 players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[133, 126, 125, 127, 140, 129, 141, 190, 124, 128]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_filter = [player_to_idx[i] for i in dic_team_active[player_team]]\n",
    "idx_to_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use it, when predicting probability on players, by taking the maximum on the set of corresponding indices only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Selection of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start filtering and selecting some qualifier features that seems inappropriate for the tasks. For instance, features related to the conditions are not available after the cleaning procedure (as they appear only at the before the game). One might still confess that, home team can be more easily guessed by the weather condition. It definitly rains more often at Lille than at Toulouse, maybe.\n",
    "\n",
    "We'll also removed features related to the referee and the VAR assistance (that was not used 2 years ago). Other features, that I found not relevant were removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 unused features\n"
     ]
    }
   ],
   "source": [
    "Conditions =  [i for i in range(45, 52)] + [i for i in range(255, 260)]\n",
    "Referee = [i for i in range(200,210)]+ [54, 226, 227, 228, 229, 246, 247, 248, 299, 303, 344] #Useless\n",
    "Referee_VAR = [i for i in range(329,345)]\n",
    "Other = [ 238, 57, 127, 144, 189, 277,299, 302,303,308, 309, 325]\n",
    "\n",
    "useless_q = sorted(Conditions + Referee + Referee_VAR + Other)\n",
    "useless_q = [str(idx) for idx in useless_q]\n",
    "print(len(useless_q), 'unused features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cbf8d8ebb845cdb34b36c3caf1f198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1495595\n"
     ]
    }
   ],
   "source": [
    "all_games = os.listdir(folder_games)\n",
    "file =  os.path.join(folder_games,all_games[0])\n",
    "_,event_df_all,q_df_all =  parse_xml_file(file)\n",
    "for idx in tqdm_notebook(range(0,len(all_games))):\n",
    "    file =  os.path.join(folder_games,all_games[idx])\n",
    "    _,event_df,q_df_ =  parse_xml_file(file)\n",
    "    q_df_all = pd.concat([q_df_all, q_df_])\n",
    "    event_df_all = pd.concat([event_df_all, event_df])\n",
    "print(q_df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 different qualifier features\n",
      "202 remaining qualifier features\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(q_df_all.qualifier_id.unique())), 'different qualifier features')\n",
    "qualifier_ids = sorted([idx for idx in q_df_all.qualifier_id.unique() if idx not in useless_q])\n",
    "print(len(qualifier_ids), 'remaining qualifier features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['34', '32', '1', '5', '61', '43', '49', '3', '50', '7', '8', '59',\n",
       "       '52', '45', '56', '44', '4', '15', '10', '13', '12', '11', '74',\n",
       "       '55', '2', '6', '27', '28', '17', '16', '51', '70', '30', '18',\n",
       "       '19', '54', '41', '37', '53', '60', '68', '40', '14', '42', '24',\n",
       "       '57', '58', '65', '20', '25'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df_all.type_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 different event features\n",
      "50 remaining event features\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(event_df_all.type_id.unique())), 'different event features')\n",
    "event_ids = sorted([idx for idx in event_df_all.type_id.unique()])\n",
    "print(len(event_ids), 'remaining event features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualifier_ids = sorted([idx for idx in q_df_all.qualifier_id.unique() if idx not in useless_q])\n",
    "len(qualifier_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qualifier_ids.pickle', 'wb') as handle:\n",
    "    pickle.dump(qualifier_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('event_ids.pickle', 'wb') as handle:\n",
    "    pickle.dump(event_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping of the qualifier_ids (between 1 and 320) to fixed indexes between 0 and 202 that will be used to create feature vectors of size 202. Same for event_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_to_idx = {}\n",
    "for i,j in enumerate(qualifier_ids):\n",
    "    q_to_idx[str(j)] = i\n",
    "    \n",
    "event_to_idx = {}\n",
    "for i,j in enumerate(event_ids):\n",
    "    event_to_idx[str(j)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_q = {}\n",
    "for i,j in enumerate(qualifier_ids):\n",
    "    idx_to_q[i] = int(j)\n",
    "    \n",
    "idx_to_event = {}\n",
    "for i,j in enumerate(event_ids):\n",
    "    idx_to_event[i] = int(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qualifier_ids), len(event_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create features vectors. We will create two of them :\n",
    " - a **team feature vector** basically composed of aggregated occurences of each event/qualifier and the pourcentage of positive outcome.\n",
    " - a **player feature vector** composed of the same components but only for 1 targeted player.\n",
    "We will had to those vectors the home/away component, the average time of the game the events occured and the period (first of second half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_vector(df_q, df_event):\n",
    "    \n",
    "    ''' Extract the aggregated feature vectors of the team (home or away). \n",
    "\n",
    "    This feature vectors basically consists in the aggregation for both team of all different \n",
    "    event occuring (related to one team at a time) and the pourcentage of positive outcome for \n",
    "    each event '''\n",
    "    \n",
    "    df_event['outcome'] = df_event['outcome'].astype(int)\n",
    "    df = df_q.join(df_event.set_index('own_id')[['outcome', 'team_id','period_id']],on=['id_event'])\n",
    "    \n",
    "    #df['outcome'] = df['outcome'].astype(int)\n",
    "    df_event['min'] = df_event['min'].astype(int)\n",
    "\n",
    "    \n",
    "    vec_list = []\n",
    "    \n",
    "    for team in range(2):\n",
    "        \n",
    "        #Choose the team (either 0 or 1)\n",
    "        df_chunk = df[df['team_id'] == str(team)]\n",
    "        #print(str(team))\n",
    "        df_event_chunk =  df_event[df_event['team_id'] == str(team)]\n",
    "        \n",
    "        #Create empty vector\n",
    "        vec_q = np.zeros(2*len(qualifier_ids)+4)\n",
    "        vec_e = np.zeros(2*len(event_ids))\n",
    "\n",
    "        # Get number of occurences\n",
    "        dic_qualifier = dict(Counter(df_chunk.qualifier_id))\n",
    "        \n",
    "        # Get number of occurences\n",
    "        dic_event = dict(Counter(df_event_chunk.type_id))\n",
    "        \n",
    "        # List of keys\n",
    "        list_keys_q = list(dic_qualifier.keys())\n",
    "        list_keys_event = list(dic_event.keys())\n",
    "        #print('Qualifier')\n",
    "        for feature in list_keys_q:\n",
    "            \n",
    "            # Get mapped index of the feature\n",
    "            mapped = q_to_idx.get(feature,None)\n",
    "            \n",
    "            if mapped != None:\n",
    "                \n",
    "                #Get the list of outcome of the feature (% of successful event)\n",
    "                l = list(df_chunk[df_chunk['qualifier_id']==feature].outcome)\n",
    "                \n",
    "                # If list isn't empty get the mean, and and to the vector : Nb_Occu, %success\n",
    "                if l != []:\n",
    "                    mean = np.round(np.mean(l),3)\n",
    "                    vec_q[mapped*2+1] = mean\n",
    "                vec_q[mapped*2] = dic_qualifier[feature]\n",
    "\n",
    "            else : \n",
    "                pass\n",
    "                #print('Q:',feature)\n",
    "        \n",
    "        l = list(df_chunk[df_chunk['qualifier_id']=='56'].value.values)  \n",
    "        if len(l) == 0 :\n",
    "            print('EMPTY LIST')\n",
    "        else :\n",
    "            c = Counter(l)\n",
    "            vec_q[-1] = c.get('Right',0) / len(l)\n",
    "            vec_q[-2] = c.get('Left',0)  / len(l)\n",
    "            vec_q[-3] = c.get('Center',0)  / len(l)\n",
    "            vec_q[-4] = c.get('Back',0)  / len(l)\n",
    "        #56 : Back, Left, Center, Right\n",
    "\n",
    "        # Same thing with the Event\n",
    "        for feature in list_keys_event:\n",
    "            mapped = event_to_idx.get(feature,None)\n",
    "            \n",
    "            if mapped != None:\n",
    "                l = list(df_event_chunk[df_event_chunk['type_id']==feature].outcome)\n",
    "\n",
    "                if l != []:\n",
    "                    mean = np.round(np.mean(l),3)\n",
    "                    vec_e[mapped*2+1] = mean\n",
    "                vec_e[mapped*2] = dic_event[feature]\n",
    "            else : \n",
    "                pass\n",
    "                #print('Ev:',feature)\n",
    "\n",
    "                \n",
    "        # Add team (home/away), the average minute, and if it is second or first half\n",
    "        vec = [int(team), int(df_event['min'].mean())/90, (df_event.period_id.unique()[0] == '2')]\n",
    "        vec = np.concatenate([vec_q, vec_e, vec])\n",
    "        vec.astype(float)\n",
    "        vec_list.append(vec)\n",
    "        \n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_vector_player(df_q, df_event, id_):\n",
    "    \n",
    "     ''' Extract the feature vectors of the player of interest. \n",
    "\n",
    "    This feature vectors basically consists in the aggregation of all different \n",
    "    event the player is involved in, and the pourcentage of positive outcome for \n",
    "    each event '''\n",
    "    \n",
    "    df_event['outcome'] = df_event['outcome'].astype(int)\n",
    "    df = df_q.join(df_event.set_index('own_id')[['outcome', 'player_id','period_id']],on=['id_event'])\n",
    "    \n",
    "    #df['outcome'] = df['outcome'].astype(int)\n",
    "    df_event['min'] = df_event['min'].astype(int)\n",
    "    \n",
    "    #Choose the player\n",
    "    df_chunk = df[df['player_id'] == str(id_)]\n",
    "    df_event_chunk =  df_event[df_event['player_id'] == str(id_)]\n",
    "    \n",
    "\n",
    "    #Create empty vector\n",
    "    vec_q = np.zeros(2*len(qualifier_ids)+4)\n",
    "    vec_e = np.zeros(2*len(event_ids))\n",
    "\n",
    "    # Get number of occurences\n",
    "    dic_qualifier = dict(Counter(df_chunk.qualifier_id))\n",
    "\n",
    "    # Get number of occurences\n",
    "    dic_event = dict(Counter(df_event_chunk.type_id))\n",
    "\n",
    "    # List of keys\n",
    "    list_keys_q = list(dic_qualifier.keys())\n",
    "    list_keys_event = list(dic_event.keys())\n",
    "    #print('Qualifier')\n",
    "    for feature in list_keys_q:\n",
    "\n",
    "        # Get mapped index of the feature\n",
    "        mapped = q_to_idx.get(feature,None)\n",
    "\n",
    "        if mapped != None:\n",
    "\n",
    "            #Get the list of outcome of the feature (% of successful event)\n",
    "            l = list(df_chunk[df_chunk['qualifier_id']==feature].outcome)\n",
    "\n",
    "            # If list isn't empty get the mean, and and to the vector : Nb_Occu, %success\n",
    "            if l != []:\n",
    "                mean = np.round(np.mean(l),3)\n",
    "                vec_q[mapped*2+1] = mean\n",
    "            vec_q[mapped*2] = dic_qualifier[feature]\n",
    "\n",
    "        else : \n",
    "            pass\n",
    "            #print('Q:',feature)\n",
    "\n",
    "    l = list(df_chunk[df_chunk['qualifier_id']=='56'].value.values)  \n",
    "    if len(l) == 0 :\n",
    "        print('EMPTY LIST PLAYER')\n",
    "        #print(vec_q[-4:], df_event_chunk.shape )\n",
    "    else :\n",
    "        c = Counter(l)\n",
    "        vec_q[-1] = c.get('Right',0) / len(l)\n",
    "        vec_q[-2] = c.get('Left',0)  / len(l)\n",
    "        vec_q[-3] = c.get('Center',0)  / len(l)\n",
    "        vec_q[-4] = c.get('Back',0)  / len(l)\n",
    "    #56 : Back, Left, Center, Right\n",
    "\n",
    "    # Same thing with the Event\n",
    "    for feature in list_keys_event:\n",
    "        mapped = event_to_idx.get(feature,None)\n",
    "\n",
    "        if mapped != None:\n",
    "            l = list(df_event_chunk[df_event_chunk['type_id']==feature].outcome)\n",
    "\n",
    "            if l != []:\n",
    "                mean = np.round(np.mean(l),3)\n",
    "                vec_e[mapped*2+1] = mean\n",
    "            vec_e[mapped*2] = dic_event[feature]\n",
    "        else : \n",
    "            pass\n",
    "            #print('Ev:',feature)\n",
    "    # Add team (home/away), the average minute, and if it is second or first half\n",
    "    vec = [int(df_event_chunk.team_id.unique()[0]),int(df_event['min'].mean())/90, int(df_event.period_id.unique()[0] == '2')]\n",
    "    vec = np.concatenate([vec_q, vec_e, vec])\n",
    "    vec.astype(float)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create a function that will generate all the possible 15 min chunks per active player in one game. To do so we will iterate on each minutes (i found not very relevant to iterate for every event, the features vectors would be too close, maybe identical, from each other), iterate on the active players that played during the game, select a 15 min chunk and anonymize all player but the active player we're are targeting. We will do this for both team. Everytime we will extract a team vector as well as a player vector and concatenate them into one vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_15min_chunk(file,itermax=10, predict='team'):\n",
    "    game_df, event_df, q_df = parse_xml_file(file)\n",
    "    home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "    #print('ID',home_team_id, away_team_id)\n",
    "    event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "    event_df['min']= event_df['min'].astype(int)\n",
    "    event_df['x']= event_df['x'].astype(float)\n",
    "    event_df['y']= event_df['y'].astype(float)\n",
    "    event_df['period_id']= event_df['period_id'].astype(int)\n",
    "\n",
    "    for it in range(1,31):\n",
    "        for half_time in range(2):\n",
    "            #half_time = np.random.random(1)\n",
    "            if half_time == 0 : \n",
    "                 t = it\n",
    "            else :\n",
    "                 t = it+45\n",
    "            df_chunk = event_df[event_df['period_id'] == (int(half_time>0.5)+1)]\n",
    "            df_chunk = df_chunk[df_chunk['min'].between(t,t+14)]\n",
    "            own_id_max = df_chunk.own_id.values[-1]\n",
    "            own_id_min = df_chunk.own_id.values[0]\n",
    "\n",
    "            q_chunk = q_df[q_df['id_event'].between(own_id_min, own_id_max)]\n",
    "            \n",
    "            if predict == 'team':\n",
    "                vec1, vec2 = get_features_vector(q_chunk, df_chunk)\n",
    "                yield vec1, away_team_id\n",
    "                yield vec2, home_team_id\n",
    "            if predict == 'player':\n",
    "                #print('PLAYER')\n",
    "                available_player = list(df_chunk.player_id.unique())\n",
    "                vec = get_features_vector(q_chunk, df_chunk)\n",
    "                for ind,team in enumerate([away_team_id, home_team_id]):\n",
    "                    for player in dic_team_active[int(team)]:\n",
    "                        if player in available_player:\n",
    "                            #print(player)\n",
    "                            #print(q_chunk.team_id.values)\n",
    "                            if df_chunk[df_chunk['player_id'] == str(player)].shape[0]==1:\n",
    "                                #print( df_chunk[df_chunk['player_id'] == str(player)])\n",
    "                                #print('empty')\n",
    "                                continue\n",
    "                            vec_player = get_features_vector_player(q_chunk, df_chunk,player)\n",
    "                            output = np.zeros(len(active_player_800))\n",
    "                            output[player_to_idx[player]] = 1\n",
    "                            yield np.concatenate([vec[ind],vec_player]), output\n",
    "    #print('file finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = r'D:\\PSG\\French Ligue One 20162017 season - Match Day 1- 19\\f24-24-2016-853139-eventdetails.xml'\n",
    "a,b = next(select_15min_chunk(test_file, predict='player'))\n",
    "print(a.shape,b.shape)\n",
    "#a[-104-3:-100-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Extract feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re going to exact the vectors once and for all, alonside the outcome expected player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdd7262171840099ef8bd40b14377ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract Vectors\n",
    "\n",
    "#create folder where we will extract vectors\n",
    "if not os.path.exists('D:/PSG/feature_vectors'):\n",
    "    os.mkdir('D:/PSG/feature_vectors')\n",
    "    \n",
    "if not os.path.exists('D:/PSG/label_vectors'):\n",
    "    os.mkdir('D:/PSG/label_vectors')\n",
    "    \n",
    "# For all games\n",
    "for file in tqdm_notebook(all_games):\n",
    "    if os.path.exists(os.path.join('D:/PSG/feature_vectors',file[:-3]+'npy')):\n",
    "        continue\n",
    "    file_path =  os.path.join(folder_games,file)\n",
    "    #print(file)\n",
    "    l_features = []\n",
    "    l_labels = []\n",
    "    # Iterate on all pairs (15 min vector of team, one seleted player)\n",
    "    for vec,label in select_15min_chunk(file_path, predict='player'):\n",
    "        l_features.append(vec[np.newaxis,:])\n",
    "        l_labels.append(label[np.newaxis,:])\n",
    "    np.save(os.path.join('D:/PSG/feature_vectors',file[:-3]+'npy'), np.concatenate(l_features,axis=0))\n",
    "    np.save(os.path.join('D:/PSG/label_vectors',file[:-3]+'npy'),  np.concatenate(l_labels,axis=0))\n",
    "    print('File saved at ', os.path.join('D:/PSG/feature_vectors',file[:-3]+'npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate between Train and Validation Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split all games between a train and a validation set. To do so, we will first map all games to the related matchday and then select a few number of matchday to become the validation set of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeb4c2bf5c740ef848d71444bf4d336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_games = os.listdir(folder_games)\n",
    "matchday_dic = {}\n",
    "for idx in tqdm_notebook(range(0,len(all_games))):\n",
    "    file =  os.path.join(folder_games,all_games[idx])\n",
    "    game_df,_,_ =  parse_xml_file(file)\n",
    "    matchday_dic[all_games[idx]] = int(game_df.matchday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchday_dic.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we choose 4 matchdays (around 40 games) across the first half of the season. I started putting aside the last 4 machdays but i found it more relevant and less biaised to choose some matchdays at different moment within the first half of the season. (Especially because some players were missing for the last 4 matchdays and the repartition was better with four non-consecutive matchdays accross the season)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 4 Valid Ratio : 0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "train_data_matchday = [1,2,3,5,6,7,9,10,11,13,14,15,17,18,19]\n",
    "valid_data_matchday = [4,8,12,16]\n",
    "\n",
    "print(len(train_data_matchday), len(valid_data_matchday), 'Valid Ratio :',len(valid_data_matchday)/19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Load Train and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we extracted all feature vectors for each game, we can directly and easily split them into a trainig and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ca2cb84926419f84ffa04d10025dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_vectors = []\n",
    "validation_labels = []\n",
    "train_vectors = []\n",
    "train_labels = []\n",
    "\n",
    "for game in tqdm_notebook(all_games):\n",
    "    feature = np.load(os.path.join('D:/PSG/feature_vectors',game[:-3]+'npy'))\n",
    "    label = np.load(os.path.join('D:/PSG/label_vectors',game[:-3]+'npy'))\n",
    "    \n",
    "    if matchday_dic[game] in valid_data_matchday :\n",
    "        validation_vectors.append(feature)\n",
    "        validation_labels.append(label)\n",
    "    else : \n",
    "        train_vectors.append(feature)\n",
    "        train_labels.append(label)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the lists into big numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_vectors = np.concatenate(validation_vectors, axis=0)\n",
    "validation_labels = np.concatenate(validation_labels, axis=0)\n",
    "train_vectors = np.concatenate(train_vectors, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Player Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some random order between our training data to avoid having very similar examplar in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (150821, 1022)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data :',train_vectors.shape)\n",
    "indices = [i for i in range(train_vectors.shape[0])]\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering any neural network architecture, let's look at some ML alogothms that might be relevant for the task.\n",
    "Let's start with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=50, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=7,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=40, random_state=0, n_jobs=7, min_samples_leaf=20, max_features=80)#, oob_score=True)\n",
    "rf.fit(train_vectors[indices],  train_labels[indices].argmax(axis=1).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_classifier.pickle', 'wb') as handle:\n",
    "    pickle.dump(rf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check the accuracy score for the training and testing part. One can notice as well that the model is largely overfiting but 12% on the test set for 230 classes is quite good in reality !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data : 0.11677615452495342\n"
     ]
    }
   ],
   "source": [
    "#score = rf.score(train_vectors, train_labels.argmax(axis=1).astype(str))\n",
    "#print('Accuracy on train data :',score)\n",
    "\n",
    "score = rf.score(validation_vectors, validation_labels.argmax(axis=1).astype(str))\n",
    "print('Accuracy on test data :',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asset of random forest method is that it gives an importance score on all features to see which are more relevant for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Player Qualifier feature 56 (0.013459)\n",
      "2. Player Qualifier feature 56 (0.013381)\n",
      "3. Player Qualifier feature 56 (0.013149)\n",
      "4. Team Qualifier feature 56 (0.011794)\n",
      "5. Team Qualifier feature 56 (0.011340)\n",
      "6. Player Qualifier feature 56 (0.010506)\n",
      "7. Team Qualifier feature 56 (0.009953)\n",
      "8. Team Qualifier feature 56 (0.009751)\n",
      "9. Player feature 509 (0.008631)\n",
      "10. Team feature 509 (0.008533)\n",
      "11. Team Qualifier feature 56 (0.007864)\n",
      "12. Team Qualifier feature 3 (0.007017)\n",
      "13. Player feature 107 (0.006741)\n",
      "14. Team Qualifier feature 157 (0.006728)\n",
      "15. Player feature 56 (0.006691)\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices_rf = np.argsort(importances)[::-1]\n",
    "for f in range(15):#train_vectors.shape[1]):\n",
    "    if indices_rf[f] // 511 == 0 :\n",
    "        if indices_rf[f] < 404 :\n",
    "            print(\"%d. Team Qualifier feature %d (%f)\" % (f + 1, idx_to_q[indices_rf[f]//2], importances[indices_rf[f]]))\n",
    "        elif indices_rf[f] < 408 :\n",
    "            print(\"%d. Team Qualifier feature %d (%f)\" % (f + 1, 56, importances[indices_rf[f]]))\n",
    "        elif indices_rf[f] <= 507 :\n",
    "            print(\"%d. Team Event feature %d (%f)\" % (f + 1, idx_to_event[indices_rf[f]//2], importances[indices_rf[f]]))\n",
    "        else :\n",
    "            print(\"%d. Team feature %d (%f)\" % (f + 1, indices_rf[f], importances[indices_rf[f]]))\n",
    "    else :\n",
    "        if indices_rf[f]-511 < 404 :\n",
    "              print(\"%d. Player feature %d (%f)\" % (f + 1, idx_to_q[(indices_rf[f]-511)//2], importances[indices_rf[f]]))\n",
    "        elif indices_rf[f]-511 < 408 :\n",
    "            print(\"%d. Player Qualifier feature %d (%f)\" % (f + 1, 56, importances[indices_rf[f]]))\n",
    "        elif indices_rf[f]-511 <= 507 :\n",
    "            print(\"%d. Player feature %d (%f)\" % (f + 1, idx_to_event[(indices_rf[f]-511)//2], importances[indices_rf[f]])) \n",
    "        else :\n",
    "            print(\"%d. Player feature %d (%f)\" % (f + 1, indices_rf[f]-511, importances[indices_rf[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, one can notice that the positition of the player (Player Qualifier 56 - Right, Left, Center and Back) is the most important feature, as well as the general orientation of the team (Team Qualifier 56 - Right, Left, Center and Back). The fact that the team plays at home or away (509) as well as the number of head pass (Qualifier 3) are also relevant. And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other cool feedback we can get is to look at the confusion matrix, to see which player are well predicted for instance but also to look at the misclassified player and confusion between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "import seaborn as sns\n",
    "#pred = rf.predict(validation_vectors)\n",
    "cm = confusion_matrix(validation_labels.argmax(axis=1).astype(str), pred, labels = [str(i) for i in range(230)])\n",
    "ax = sns.heatmap(cm[:50,:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yoan Cardinale --> Vincent Enyeama\n",
      "Hiroki Sakai --> Steven Moreira\n",
      "Thiago Motta --> Marco Verratti\n",
      "Vincent Le Goff --> Yoann Andreu\n",
      "Dante Bonfim Costa Santos --> Malang Sarr\n"
     ]
    }
   ],
   "source": [
    "tot =  validation_labels.sum(axis=0)\n",
    "for i in range(230):\n",
    "    #print(player_names.get(str(idx_to_player[i]),'No Match'),cm[i,i]/tot[i])\n",
    "    for j in range(230):\n",
    "        if i==j :\n",
    "            continue\n",
    "        else :\n",
    "            if cm[j,i]>50 or cm[j,i]>50:\n",
    "                print(player_names.get(str(idx_to_player[i]),'No Match'), '-->', player_names.get(str(idx_to_player[j]),'No Match'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, one can see that the most confused pairs of player are player that look alike and that have very closed profile (a priori). For instance Verratti and Motta plays more or less at the same position in the same team and are player that passes a lot for instance.\n",
    "\n",
    "One can see that Dante and Sarr are also confused but it seems coherent as they are the two center back of Nice's team ! Similarly, Yohan Cardinale and Enyemama are both Goalkeepers !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Verratti_Motta.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Match 0.1087866108786611\n",
      "Jérémy Morel 0.016666666666666666\n",
      "Corentin Tolisso 0.0763888888888889\n",
      "Maxime Gonalons 0.06432748538011696\n",
      "Guy NDy Assembe 0.1111111111111111\n",
      "Mapou Yanga-Mbiwa 0.03333333333333333\n",
      "Sergi Darder 0.09230769230769231\n",
      "Anthony Lopes 0.20253164556962025\n",
      "Rafael Pereira da Silva 0.03333333333333333\n",
      "Maxwel Cornet 0.05102040816326531\n",
      "Nabil Fekir 0.12777777777777777\n",
      "Youssef Aït Bennasser 0.02857142857142857\n",
      "Joffrey Cuffaut 0.016666666666666666\n",
      "Benoit Pedretti 0.08888888888888889\n",
      "Alexandre Lacazette 0.08888888888888889\n",
      "Tobias Badila 0.3111111111111111\n",
      "Loïc Puyo 0.0\n",
      "Vincent Pajot 0.0\n",
      "Youssouf Sabaly 0.19583333333333333\n",
      "Malcom Filipe Silva de Oliveira 0.13690476190476192\n"
     ]
    }
   ],
   "source": [
    "tot =  validation_labels.sum(axis=0)\n",
    "for i in range(20):\n",
    "    print(player_names.get(str(idx_to_player[i]),'No Match'),cm[i,i]/tot[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ángel Di María 0.3959731543624161\n",
      "Yoan Cardinale 0.3926940639269407\n",
      "Marco Verratti 0.3855421686746988\n",
      "Layvin Kurzawa 0.34823529411764703\n",
      "Alphonse Areola 0.3419354838709678\n",
      "Lucas Pedro Alves de Lima 0.3089700996677741\n",
      "Steve Mounie 0.3079526226734348\n",
      "Ricardo Domingos Barbosa Pereira 0.3039014373716633\n",
      "François Moubandje 0.296\n",
      "Tobias Badila 0.2955145118733509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "F1 =  f1_score(validation_labels.argmax(axis=1).astype(str), pred, labels = [str(i) for i in range(230)], average=None) \n",
    "f1_sorted = np.argsort(F1)[::-1]\n",
    "\n",
    "for i in f1_sorted[:10]:\n",
    "    print(player_names.get(str(idx_to_player[i]),'No Match'),F1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algortihm struggles to find some players (here we have 0 accuracy on those specific ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomas Hubocan 0.0\n",
      "Issiaga Sylla 0.0\n",
      "Pierrick Capelle 0.0\n",
      "Marko Basa 0.0\n",
      "Gaël Danic 0.0\n",
      "Kévin Monnet-Paquet 0.0\n",
      "Moustapha Diallo 0.0\n",
      "Léo Lacroix 0.0\n",
      "Yann Bodiger 0.0\n",
      "Ramy Bensebaini 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in f1_sorted[-10:]:\n",
    "    print(player_names.get(str(idx_to_player[i]),'No Match'),F1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other way to predict the players would be to build a deep neural network.\n",
    "To do so, I tried to build a 2-tower neural nets, that deals with the team feature vectors separatly from the player one, at first, and then concatenate the two, followed by a simple dense layer. I didn't have a lot a time to work on the architecture, this would be a lead of improvement to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(os.path.join('D:/PSG/', 'team_prediction.h5'),\n",
    "                    monitor='val_loss', verbose=1,\n",
    "                    save_best_only=True, save_weights_only=True,\n",
    "                    mode='auto', period=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 511)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 511)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 511)          2044        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 511)          2044        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 10)           5120        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 20)           10240       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 10)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 20)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 38)           0           dropout_12[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 230)          8970        merge_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 28,418\n",
      "Trainable params: 26,374\n",
      "Non-trainable params: 2,044\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_player = Input(shape=(511, ))\n",
    "y_player = BatchNormalization(name='batch_normalization_1')(x_player)\n",
    "y_player = Dense(20,  activation='sigmoid')(y_player)\n",
    "y_player = Dropout(0.5)(y_player)\n",
    "player_tower = Model(inputs=x_player, outputs=y_player)\n",
    "\n",
    "x_player_pps = Input(shape=(4, ))\n",
    "player_tower_pps = Model(inputs=x_player_pps, outputs=x_player_pps)\n",
    "\n",
    "\n",
    "x_team = Input(shape=(511, ))\n",
    "y_team = BatchNormalization(name='batch_normalization_2')(x_team)\n",
    "y_team = Dense(10,  activation='sigmoid')(y_team)\n",
    "y_team = Dropout(0.5)(y_team)\n",
    "team_tower = Model(inputs=x_team, outputs=y_team)\n",
    "\n",
    "x_team_pps = Input(shape=(4, ))\n",
    "team_tower_pps = Model(inputs=x_team_pps, outputs=x_team_pps)\n",
    "\n",
    "merged = concatenate([team_tower.output,player_tower.output, team_tower_pps.output, player_tower_pps.output], axis=1, name ='merge_1')\n",
    "merged = Dense(230, activation='sigmoid')(merged)\n",
    "\n",
    "final_model = Model([team_tower.input,player_tower.input, team_tower_pps.input, player_tower_pps.input], merged)\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit([train_vectors[indices,:511], train_vectors[indices,511:], train_vectors[indices,404:408],\n",
    "                 train_vectors[indices,915:919]],\n",
    "          train_labels[indices], \n",
    "          batch_size=128,\n",
    "          epochs= 100, \n",
    "          verbose=2, \n",
    "          validation_data = ([validation_vectors[:,:511],validation_vectors[:,511:],  validation_vectors[:,404:408],\n",
    "                 validation_vectors[:,915:919]], validation_labels), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```- 17s - loss: 3.2729 - acc: 0.1918 - val_loss: 3.8729 - val_acc: 0.1222```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple neural nets with two Dense layers. To explore as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(1022, ))\n",
    "y = BatchNormalization(name='batch_normalization_1')(x)\n",
    "#y = Dropout(0.5)(x)\n",
    "y = Dense(50,  activation='sigmoid')(y)\n",
    "y = Dropout(0.8)(y)\n",
    "y = Dense(230, activation='sigmoid')(y)\n",
    "final_model = Model(inputs=x, outputs = y)\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "final_model.summary()\n",
    "\n",
    "final_model.fit([train_vectors[indices]],\n",
    "          train_labels[indices], \n",
    "          batch_size=128,\n",
    "          epochs= 100, \n",
    "          verbose=2, \n",
    "          validation_data = (validation_vectors, validation_labels), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```- 10s - loss: 3.9816 - acc: 0.0748 - val_loss: 3.9198 - val_acc: 0.1060```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict Next Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is to predict the next event that will occur avec the 15 min chunk. By next event, I mean will need to predict the team concerned (0 away / 1 home) and the position (x,y) of this event. I choose, for sake of simplicity, to do those two tasks seperatly. Let's start with the next team prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Model n°1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt, I build a batch generator that aims at selecting the 10 last events and to build a vector of size (10,4) that provides information on the team, teh position x, y and the type of event id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(mode='Train', batch_size = 64):\n",
    "    \n",
    "    all_games = os.listdir(folder_games)\n",
    "    idx = 0\n",
    "    batch_input = np.zeros((batch_size, 10, 4))\n",
    "\n",
    "    batch_output = np.zeros((batch_size))\n",
    "    a = 0\n",
    "    if mode=='Train' : \n",
    "        def update_idx(idx):\n",
    "            return (idx+1)%int((len(all_games)*0.8))\n",
    "    if mode=='Val' : \n",
    "        def update_idx(idx):\n",
    "            return (idx+1)%int((len(all_games)*0.2)) + int((len(all_games)*0.8)) \n",
    "    while True : \n",
    "        file =  os.path.join(folder_games,all_games[idx])\n",
    "        idx = update_idx(idx)\n",
    "        game_df, event_df, q_df = parse_xml_file(file)\n",
    "        home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "\n",
    "        event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "        event_df['min']= event_df['min'].astype(int)\n",
    "        event_df['x']= event_df['x'].astype(float)\n",
    "        event_df['y']= event_df['y'].astype(float)\n",
    "        maxx = event_df['x'].max()\n",
    "        maxy = event_df['y'].max()\n",
    "        event_df['period_id']= event_df['period_id'].astype(int)\n",
    "        event_df = event_df[event_df['min'].between(2,90)]\n",
    "        \n",
    "        chunk = [t for t in range(1,event_df.shape[0]-20, 10)]\n",
    "        \n",
    "        for t in chunk : \n",
    " \n",
    "            df_chunk = event_df.loc[event_df.index[t:t+10]]\n",
    "            output = int(event_df.loc[event_df.index[t+10],'team_id'])\n",
    "        \n",
    "            df_chunk = df_chunk[['team_id','x', 'y','type_id']]\n",
    "\n",
    "            batch_input[a,:,:] = np.array(df_chunk)\n",
    "            batch_input[a,:,1] /= maxx\n",
    "            batch_input[a,:,2] /= maxy\n",
    "            batch_output[a] = output\n",
    "\n",
    "            a+=1\n",
    "\n",
    "            if a % batch_size == 0:\n",
    "                yield [batch_input[:,:,3:].reshape((batch_size,10)),batch_input[:,:,:3]] , batch_output\n",
    "\n",
    "                batch_input = np.zeros((batch_size, 10, 4))\n",
    "                batch_output = np.zeros((batch_size))\n",
    "                a=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is very basic. Two tower. One for the event id part that aims at creating an embeddings of events out of the sequences of events. One can think about words in a sentence for instance. Thise is a very similar idea. I hope the embedding will capture interesting relationship between events. Then I'll concatente the embedded events alongside the sequences of (x,y,team) and put it into an LSTM layer followed by two dense layers. \n",
    "\n",
    "This gives approximetly 80% of accuracy on a set of events occurung during 20% of the unseen games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(os.path.join('D:/PSG/', 'team_prediction.h5'),\n",
    "                    monitor='val_loss', verbose=1,\n",
    "                    save_best_only=True, save_weights_only=True,\n",
    "                    mode='auto', period=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 3)             0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 10)            770       \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 10)       770         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 10, 13)       0           embedding_1[0][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 10)           960         merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 5)            55          lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1)            6           dense_47[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,791\n",
      "Trainable params: 1,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "coord_in = Input(shape=(10,3,),name='input_2')\n",
    "#x = BatchNormalization(name='batch_normalization_1')(coord_in)\n",
    "#x = Dense(10, activation='relu',name='dense_1')(x)           \n",
    "coord = Model(inputs=coord_in, outputs=coord_in)\n",
    "coord.summary()\n",
    "\n",
    "# Caption Encoder\n",
    "caption_in = Input(shape=(10, ), name='input_1')\n",
    "y = Embedding(77, 10, input_length=10, name ='embedding_1')(caption_in)\n",
    "#y = LSTM(10,return_sequences=True)(y)\n",
    "#y = BatchNormalization(name='batch_normalization_95')(y)\n",
    "\n",
    "type_event = Model(inputs=caption_in, outputs=y)\n",
    "type_event.summary()\n",
    "\n",
    "merged = concatenate([type_event.output, coord.output], axis=2, name ='merge_1')\n",
    "merged = LSTM(10)(merged)\n",
    "#merged = Dropout(0.3)(merged)\n",
    "merged = Dense(5, activation='sigmoid')(merged)\n",
    "#merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "final_model = Model([type_event.input,coord.input], merged)\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.optimizer = Adam(lr=1e-4)\n",
    "\n",
    "final_model.fit_generator(\n",
    "            generator=batch_generator('Train', 32),\n",
    "            steps_per_epoch=400,\n",
    "            epochs=100,\n",
    "            verbose=2,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=batch_generator('Val'),\n",
    "            validation_steps = 68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```- 33s - loss: 0.4377 - acc: 0.7993 - val_loss: 0.4645 - val_acc: 0.7911```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another try, without the embedding tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(mode='Train', batch_size = 64):\n",
    "    \n",
    "    all_games = os.listdir(folder_games)\n",
    "    idx = 0\n",
    "    batch_input = np.zeros((batch_size, 10, 80))\n",
    "\n",
    "    batch_output = np.zeros((batch_size,1))\n",
    "    a = 0\n",
    "    if mode=='Train' : \n",
    "        def update_idx(idx):\n",
    "            return (idx+1)%int((len(all_games)*0.8))\n",
    "    if mode=='Val' : \n",
    "        def update_idx(idx):\n",
    "            return (idx+1)%int((len(all_games)*0.2)) + int((len(all_games)*0.8)) \n",
    "    while True : \n",
    "        file =  os.path.join(folder_games,all_games[idx])\n",
    "        idx = update_idx(idx)\n",
    "        game_df, event_df, q_df = parse_xml_file(file)\n",
    "        home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "\n",
    "        event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "        event_df['min']= event_df['min'].astype(int)\n",
    "        event_df['x']= event_df['x'].astype(float)\n",
    "        event_df['y']= event_df['y'].astype(float)\n",
    "        maxx = event_df['x'].max()\n",
    "        maxy = event_df['y'].max()\n",
    "        event_df['period_id']= event_df['period_id'].astype(int)\n",
    "        event_df = event_df[event_df['min'].between(2,90)]\n",
    "        \n",
    "        chunk = [t for t in range(1,event_df.shape[0]-30, 10)]\n",
    "        \n",
    "        for t in chunk : \n",
    " \n",
    "            df_chunk = event_df.loc[event_df.index[t:t+10]]\n",
    "            output = int(event_df.loc[event_df.index[t+10],'team_id'])\n",
    "        \n",
    "            df_chunk = df_chunk[['team_id','x', 'y','type_id']]\n",
    "\n",
    "            batch_input[a,:,:3] =  df_chunk[['team_id','x', 'y']]\n",
    "            batch_input[a,:,1] /= maxx\n",
    "            batch_input[a,:,2] /= maxy\n",
    "            \n",
    "            input_ = np.zeros((10,77))\n",
    "            for i,val in enumerate(df_chunk.type_id.values) :\n",
    "                input_[i,int(val)] = 1\n",
    "\n",
    "            batch_input[a,:,3:] = input_\n",
    "            batch_output[a,0] = output\n",
    "\n",
    "            a+=1\n",
    "\n",
    "            if a % batch_size == 0:\n",
    "                #print('yiel', batch_input.shape,batch_output.shape )\n",
    "                yield batch_input, batch_output\n",
    "\n",
    "                batch_input = np.zeros((batch_size, 10, 80))\n",
    "                batch_output = np.zeros((batch_size,1))\n",
    "                a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10, 80)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(batch_generator(mode='Train', batch_size = 64))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(10,80,),name='input_1')\n",
    "y = LSTM(20)(x)\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = Dense(5, activation='sigmoid')(y)\n",
    "merged = Dropout(0.2)(merged)\n",
    "y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "final_model = Model(inputs=x, outputs=y)\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "final_model.fit_generator(\n",
    "            generator=batch_generator('Train', 128),\n",
    "            steps_per_epoch=100,\n",
    "            epochs=100,\n",
    "            verbose=2,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=batch_generator('Val'),\n",
    "            validation_steps = 68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```- 42s - loss: 0.4125 - acc: 0.8156 - val_loss: 0.4489 - val_acc: 0.7971```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Model n°2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model, that aim at combining info on the 10 last events as well as a feature vectors of both team during the 15 last minutes. First of all, we re gonna extract a feature vector from the last (approximatly) 15 last minutes with number of occurences and corresponding ratio of positive outcome associated to an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_file(file):\n",
    "    \n",
    "    batch_input = []\n",
    "    batch_vec = []\n",
    "    batch_output = []\n",
    "    \n",
    "\n",
    "    game_df, event_df, q_df = parse_xml_file(file)\n",
    "    event_df['outcome'] = event_df['outcome'].astype(int)\n",
    "    home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "    #print('ID',home_team_id, away_team_id)\n",
    "    event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "    event_df['min']= event_df['min'].astype(int)\n",
    "    event_df['x']= event_df['x'].astype(float)\n",
    "    event_df['y']= event_df['y'].astype(float)\n",
    "    event_df['period_id']= event_df['period_id'].astype(int)\n",
    "\n",
    "    maxx = event_df['x'].max()\n",
    "    maxy = event_df['y'].max()\n",
    "\n",
    "    event_df = event_df[event_df['min'].between(1,90)]\n",
    "\n",
    "    chunk = [t for t in range(1,event_df.shape[0]-310, 5)]\n",
    "\n",
    "    for t in chunk : \n",
    "\n",
    "        # Select the 300 events before the 10 anonymised event\n",
    "        past_df = event_df.loc[event_df.index[t:t+300]]\n",
    "        # Select the 10 last ananumised event\n",
    "        df_chunk = event_df.loc[event_df.index[t+300:t+310]]\n",
    "        # Get output\n",
    "        output = int(event_df.loc[event_df.index[t+310],'team_id'])\n",
    "\n",
    "        # We're gonna collect the two vectors of the past events aggregated by team \n",
    "        vec = []\n",
    "        for team in range(2) :\n",
    "            # Select the concerned team\n",
    "            df_event_chunk = past_df[past_df['team_id']==str(team)]\n",
    "            #print(past_df.shape,df_event_chunk.shape, past_df.team_id.unique())\n",
    "            #Get occurances of each event type\n",
    "            dic_event = dict(Counter(df_event_chunk.type_id))\n",
    "            #print(dic_event)\n",
    "            # List of keys\n",
    "            list_keys_event = list(dic_event.keys())\n",
    "            vec_e = np.zeros(50*2)\n",
    "\n",
    "            for feature in list_keys_event:\n",
    "                mapped = event_to_idx.get(feature,None)\n",
    "                if mapped != None:\n",
    "                    l = list(df_event_chunk[df_event_chunk['type_id']==feature].outcome)\n",
    "\n",
    "                    if l != []:\n",
    "                        mean = np.round(np.mean(l),3)\n",
    "                        vec_e[mapped*2+1] = mean\n",
    "                        #print(mean)\n",
    "                    vec_e[mapped*2] = dic_event[feature]\n",
    "                else : \n",
    "                    pass\n",
    "            vec.append(vec_e)\n",
    "        # Vec of event with outcome ratio\n",
    "        vec = np.concatenate(vec, axis=0)\n",
    "\n",
    "        batch_vec.append(vec[np.newaxis,:])\n",
    "        \n",
    "        df_chunk = df_chunk[['team_id','x', 'y','type_id']]\n",
    "        batch_input_vec = np.array(df_chunk)\n",
    "        batch_input_vec[:,1] /= maxx\n",
    "        batch_input_vec[:,2] /= maxy\n",
    "        batch_input.append(batch_input_vec[np.newaxis,:,:])\n",
    "        batch_output.append(output)\n",
    "        \n",
    "    return np.concatenate(batch_vec, axis=0), np.concatenate(batch_input, axis=0), np.array(batch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accelerate the training, we 're gonna ectracte feature vectors for all games once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in tqdm_notebook(all_games):\n",
    "    if os.path.exists(os.path.join('D:/PSG/batch',file[:-4]+'_vec.npy')):\n",
    "        continue\n",
    "    file_path =  os.path.join(folder_games,file)\n",
    "    # Iterate on all pairs (15 min vector of team, one seleted player)\n",
    "    vec, input_, output = get_array_file(file_path)\n",
    "    np.save(os.path.join('D:/PSG/batch',file[:-4]+'_vec.npy'),vec)\n",
    "    np.save(os.path.join('D:/PSG/batch',file[:-4]+'_input.npy'), input_)\n",
    "    np.save(os.path.join('D:/PSG/batch',file[:-4]+'_output.npy'), output)\n",
    "    print('File saved at ', os.path.join('D:/PSG/batch',file[:-3]+'npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485a84995b1444fa83caf4def171334f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_vectors = []\n",
    "validation_input = []\n",
    "validation_labels = []\n",
    "train_vectors = []\n",
    "train_input = []\n",
    "train_labels = []\n",
    "\n",
    "for game in tqdm_notebook(all_games):\n",
    "    if not os.path.exists(os.path.join('D:/PSG/batch',game[:-4]+'_vec.npy')):\n",
    "        continue\n",
    "    vec = np.load(os.path.join('D:/PSG/batch',game[:-4]+'_vec.npy'))\n",
    "    input_ = np.load(os.path.join('D:/PSG/batch',game[:-4]+'_input.npy'))\n",
    "    label = np.load(os.path.join('D:/PSG/batch',game[:-4]+'_output.npy'))\n",
    "\n",
    "    if matchday_dic[game] in valid_data_matchday :\n",
    "        validation_vectors.append(vec)\n",
    "        validation_input.append(input_)\n",
    "        validation_labels.append(label)\n",
    "    else : \n",
    "        train_vectors.append(vec)\n",
    "        train_input.append(input_)\n",
    "        train_labels.append(label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_vectors = np.concatenate(validation_vectors, axis=0)\n",
    "validation_input = np.concatenate(validation_input, axis=0)\n",
    "validation_labels = np.concatenate(validation_labels, axis=0)\n",
    "\n",
    "train_vectors = np.concatenate(train_vectors, axis=0)\n",
    "train_input = np.concatenate(train_input, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 3)             0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 10)            770       \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 10)       770         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Concatenate)           (None, 10, 13)       0           embedding_1[0][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 20)           2720        merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Concatenate)           (None, 220)          0           lstm_42[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 5)            1105        merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 1)            6           dense_94[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "coord_in = Input(shape=(10,3,),name='input_2')       \n",
    "coord = Model(inputs=coord_in, outputs=coord_in)\n",
    "coord.summary()\n",
    "\n",
    "# Caption Encoder\n",
    "caption_in = Input(shape=(10, ), name='input_1')\n",
    "y = Embedding(77, 10, input_length=10, name ='embedding_1')(caption_in)\n",
    "type_event = Model(inputs=caption_in, outputs=y)\n",
    "type_event.summary()\n",
    "\n",
    "vec_in = Input(shape=(200, ), name='input_3')\n",
    "vec_model = Model(inputs=vec_in, outputs=vec_in)\n",
    "\n",
    "\n",
    "merged = concatenate([type_event.output, coord.output, ], axis=2, name ='merge_1')\n",
    "merged = LSTM(20)(merged)\n",
    "merged = concatenate([merged, vec_model.output ], axis=1, name ='merge_2')\n",
    "#merged = Dropout(0.3)(merged)\n",
    "merged = Dense(5, activation='sigmoid')(merged)\n",
    "#merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "final_model = Model([type_event.input,coord.input, vec_model.output], merged)\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.optimizer = Adam(lr=1e-4)\n",
    "final_model.fit(\n",
    "            [train_input[:,:,3],train_input[:,:,:3],train_vectors],\n",
    "            train_labels,\n",
    "            epochs=25,\n",
    "            verbose=2,\n",
    "            batch_size=16,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=([validation_input[:,:,3],validation_input[:,:,:3], validation_vectors], validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```85s - loss: 0.4167 - acc: 0.8126 - val_loss: 0.4435 - val_acc: 0.8061```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 20338, 1: 21097})"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predict Next Coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last model, the one that will predict the next (x,y) coordinate. Unfortunetly, I didn't get much time to work deeper on this one. It looks like the previous model for the prediction of the next team. Whereas here, we gonna change the metric with MSE and use linear or relu activations instead of sigmoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_array_file(file):\n",
    "    \n",
    "    batch_input = []\n",
    "    batch_output = []\n",
    "\n",
    "    game_df, event_df, q_df = parse_xml_file(file)\n",
    "    event_df['outcome'] = event_df['outcome'].astype(int)\n",
    "    home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "    event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "    event_df['min']= event_df['min'].astype(int)\n",
    "    event_df['x']= event_df['x'].astype(float)\n",
    "    event_df['y']= event_df['y'].astype(float)\n",
    "    event_df['team_id']= event_df['team_id'].astype(int)\n",
    "    maxx = event_df['x'].max()\n",
    "    maxy = event_df['y'].max()\n",
    "\n",
    "    event_df = event_df[event_df['min'].between(1,90)]\n",
    "\n",
    "    chunk = [t for t in range(1,event_df.shape[0]-10, 3)]\n",
    "\n",
    "    for t in chunk : \n",
    "\n",
    "        # Select the 10 last ananumised event\n",
    "        df_chunk = event_df.loc[event_df.index[t:t+10]]\n",
    "        # Get output\n",
    "        output = np.array(event_df.loc[event_df.index[t+10],['y','x']])\n",
    "        df_chunk = df_chunk[['team_id','x', 'y','type_id']]\n",
    "        \n",
    "        input_ = np.zeros((10,53))\n",
    "        for i,val in enumerate(df_chunk.type_id.values) :\n",
    "            input_[i,event_to_idx[val]] = 1\n",
    "        input_[:,-3] = df_chunk.x.values\n",
    "        input_[:,-2] = df_chunk.y.values\n",
    "        input_[:,-1] = df_chunk.team_id.values\n",
    "\n",
    "        #batch_input_vec[:,1] /= maxx\n",
    "        #batch_input_vec[:,2] /= maxy\n",
    "        batch_input.append(input_[np.newaxis,:,:])\n",
    "        batch_output.append(output)\n",
    "        \n",
    "    return np.concatenate(batch_input, axis=0), np.array(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((526, 10, 53), (526, 2))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = get_array_file(test_file)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in tqdm_notebook(all_games):\n",
    "    if os.path.exists(os.path.join('D:/PSG/batch_xy',file[:-4]+'_input.npy')):\n",
    "        continue\n",
    "    file_path =  os.path.join(folder_games,file)\n",
    "    # Iterate on all pairs (15 min vector of team, one seleted player)\n",
    "    input_, output = get_array_file(file_path)\n",
    "    np.save(os.path.join('D:/PSG/batch_xy',file[:-4]+'_input.npy'), input_)\n",
    "    np.save(os.path.join('D:/PSG/batch_xy',file[:-4]+'_output.npy'), output)\n",
    "    print('File saved at ', os.path.join('D:/PSG/batch_xy',file[:-3]+'npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0687f24b235433798fdd601627e8de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_input = []\n",
    "validation_labels = []\n",
    "train_input = []\n",
    "train_labels = []\n",
    "\n",
    "for game in tqdm_notebook(all_games):\n",
    "    if not os.path.exists(os.path.join('D:/PSG/batch_xy',game[:-4]+'_input.npy')):\n",
    "        continue\n",
    "    input_ = np.load(os.path.join('D:/PSG/batch_xy',game[:-4]+'_input.npy'))\n",
    "    label = np.load(os.path.join('D:/PSG/batch_xy',game[:-4]+'_output.npy'))\n",
    "\n",
    "    if matchday_dic[game] in valid_data_matchday :\n",
    "        validation_input.append(input_)\n",
    "        validation_labels.append(label)\n",
    "    else : \n",
    "        train_input.append(input_)\n",
    "        train_labels.append(label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = np.concatenate(validation_input, axis=0)\n",
    "validation_labels = np.concatenate(validation_labels, axis=0)\n",
    "\n",
    "train_input = np.concatenate(train_input, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(os.path.join('D:/PSG/', 'x_y.h5'),\n",
    "                    monitor='val_loss', verbose=1,\n",
    "                    save_best_only=True, save_weights_only=True,\n",
    "                    mode='auto', period=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10, 53)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 53)            212       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10)                2560      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 2,794\n",
      "Trainable params: 2,688\n",
      "Non-trainable params: 106\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(10,53,),name='input_2')       \n",
    "y = BatchNormalization(name='batch_normalization_1')(x)\n",
    "y = LSTM(10)(y)\n",
    "#y = Dense(5, activation='relu')(y)\n",
    "#merged = Dropout(0.2)(merged)\n",
    "y = Dense(2, activation='relu')(y)\n",
    "\n",
    "final_model = Model(inputs=x, outputs=y)\n",
    "final_model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.fit(train_input,\n",
    "            #[train_input[:,:,3],train_input[:,:,:3]],\n",
    "            train_labels,\n",
    "            epochs=25,\n",
    "            verbose=2,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            #validation_data=([validation_input[:,:,3],validation_input[:,:,:3]], validation_labels))\n",
    "            validation_data=(validation_input, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 25/25\n",
    " - 87s - loss: 543.4141 - mean_squared_error: 543.4141 - mean_absolute_error: 17.4937 - val_loss: 536.6297 - val_mean_squared_error: 536.6297 - val_mean_absolute_error: 17.2772```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(os.path.join('D:/PSG/', 'x_y_other.h5'),\n",
    "                    monitor='val_loss', verbose=1,\n",
    "                    save_best_only=True, save_weights_only=True,\n",
    "                    mode='auto', period=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(name='batch_normalization_1'))\n",
    "model.add(LSTM(20, input_shape=(10,80)))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_input,\n",
    "            train_labels,\n",
    "            epochs=25,\n",
    "            verbose=2,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(validation_input, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```- 80s - loss: 627.2031 - mean_squared_error: 627.2031 - val_loss: 544.6281 - val_mean_squared_error: 544.6281```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparaison with or without team trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the difference when prediction without any knowledge on the team or if we know the team beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_team(event_df, q_df, player_id):\n",
    "\n",
    "    ''' Get the id of the player of interest '''\n",
    "        \n",
    "    q_id_with_players = [7, 30 , 53, 194, 281]\n",
    "    teams= {}\n",
    "\n",
    "    for i in q_id_with_players:\n",
    "        df_chunk =  q_df[q_df['qualifier_id']==str(i)] \n",
    "        for row in df_chunk.iterrows():\n",
    "            id_event = row[1]['id_event']\n",
    "            value = row[1]['value'].split(', ')\n",
    "            #print(id_event,value)\n",
    "            id_team = event_df[event_df['own_id'] ==  id_event].team_id.values[0]\n",
    "            teams[id_team] = teams.get(id_team,[]) + value\n",
    "            \n",
    "    team_dic = {}\n",
    "    for t in range(2) : \n",
    "        teams[t] = set(teams.get(str(t),[]))\n",
    "        team, val = get_team(teams[t])\n",
    "        if val > 1 :\n",
    "            team_dic[str(t)] = team\n",
    "\n",
    "    team_id = event_df[event_df['player_id'] == player_id].team_id.unique()[0]\n",
    "    player_team = team_dic.get(team_id, 'None')\n",
    "    return player_team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to get the information : Is the team known or not in the same order as our feature vectors !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_15min_chunk(file,itermax=10, predict='team'):\n",
    "    \n",
    "    game_df, event_df, q_df = parse_xml_file(file)\n",
    "    home_team_id, away_team_id = game_df['home_team_id'][0], game_df['away_team_id'][0]\n",
    "    event_df['team_id'] = event_df['team_id'].map({home_team_id: '1', away_team_id:'0'})\n",
    "    event_df['min']= event_df['min'].astype(int)\n",
    "    event_df['x']= event_df['x'].astype(float)\n",
    "    event_df['y']= event_df['y'].astype(float)\n",
    "    event_df['period_id']= event_df['period_id'].astype(int)\n",
    "    \n",
    "    for it in range(1,31):\n",
    "        for half_time in range(2):\n",
    "            if half_time == 0 : \n",
    "                 t = it\n",
    "            else :\n",
    "                 t = it+45\n",
    "            df_chunk = event_df[event_df['period_id'] == (int(half_time>0.5)+1)]\n",
    "            df_chunk = df_chunk[df_chunk['min'].between(t,t+14)]\n",
    "            own_id_max = df_chunk.own_id.values[-1]\n",
    "            own_id_min = df_chunk.own_id.values[0]\n",
    "\n",
    "            q_chunk = q_df[q_df['id_event'].between(own_id_min, own_id_max)]\n",
    "            \n",
    "            if predict == 'player':\n",
    "\n",
    "                available_player = list(df_chunk.player_id.unique())\n",
    "                for ind,team in enumerate([away_team_id, home_team_id]):\n",
    "                    for player in dic_team_active[int(team)]:\n",
    "                        if player in available_player:\n",
    "                            if df_chunk[df_chunk['player_id'] == str(player)].shape[0]==1:\n",
    "                                continue\n",
    "                                \n",
    "                            yield filter_team(df_chunk,q_chunk, str(player))\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save it once and for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da08743c3074f719ec4d4597b4b3f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ex =r'D:\\PSG\\French Ligue One 20162017 season - Match Day 1- 19\\f24-24-2016-853139-eventdetails.xml'\n",
    "l_features = []\n",
    "for file in tqdm_notebook(all_games):\n",
    "    if os.path.exists(os.path.join('D:/PSG/team_a_priori',file[:-3]+'npy')):\n",
    "        continue\n",
    "    file_path =  os.path.join(folder_games,file)\n",
    "    l_features = []\n",
    "    # Iterate on all pairs (15 min vector of team, one seleted player)\n",
    "    for team in select_15min_chunk(file_path, predict='player'):\n",
    "        l_features.append(str(team))\n",
    "    l_features = np.array(l_features)\n",
    "    np.save(os.path.join('D:/PSG/team_a_priori',file[:-3]+'npy'),l_features)\n",
    "    #print('File saved at ', os.path.join('D:/PSG/feature_vectors',file[:-3]+'npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reconstruct the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1f7c971c824f28a1594d8f54e75ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_vectors = []\n",
    "validation_team = []\n",
    "validation_labels = []\n",
    "\n",
    "for game in tqdm_notebook(all_games):\n",
    "    if not os.path.exists(os.path.join('D:/PSG/batch',game[:-4]+'_vec.npy')):\n",
    "        continue\n",
    "        \n",
    "    if matchday_dic[game] in valid_data_matchday :\n",
    "\n",
    "        vec = np.load(os.path.join('D:/PSG/feature_vectors',game[:-4]+'.npy'))\n",
    "        team_vec = np.load(os.path.join('D:/PSG/team_a_priori',game[:-4]+'.npy'))\n",
    "        label = np.load(os.path.join('D:/PSG/label_vectors',game[:-4]+'.npy'))\n",
    "    \n",
    "        validation_vectors.append(vec)\n",
    "        validation_team.append(team_vec)\n",
    "        validation_labels.append(label)\n",
    "    else : \n",
    "\n",
    "        pass\n",
    "        \n",
    "validation_vectors = np.concatenate(validation_vectors, axis=0)\n",
    "validation_labels = np.concatenate(validation_labels, axis=0)\n",
    "validation_team = np.concatenate(validation_team, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check we have the exact number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40796, 1022), (40796, 230), (40796,))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_vectors.shape, validation_labels.shape, validation_team.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_trick import filtered_indices_team\n",
    "\n",
    "from player_prediction import get_feature_vector, load_player_model#, player_names\n",
    "player_model = load_player_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can predict the id player. \n",
    "\n",
    "- In the case we don't know anything about the team, we use our general algorithm which returns the argmax of the probability vectors on 230 classes\n",
    "- If we know the team, we can apply the argmax only on the set of active player in the corresponding team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files 40796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7875f3cdfcfd4c9185bc04245c546393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40796), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = []\n",
    "print('Number of validation files', validation_vectors.shape[0])\n",
    "for i in tqdm_notebook(range(validation_vectors.shape[0])):#,validation_vectors.shape[0])):\n",
    "    vec_player = validation_vectors[i,:]\n",
    "    player_team = validation_team[i]\n",
    "    \n",
    "    if player_team == 'None':\n",
    "        filtered_indices = [i for i in range(230)]\n",
    "    else : \n",
    "        filtered_indices = [player_to_idx[i] for i in dic_team_active[int(player_team)]]\n",
    "    #filtered_indices = sorted(filtered_indices)\n",
    "\n",
    "    if len(filtered_indices) == 230 : \n",
    "    # Predict the id of the player\n",
    "        res_player = player_model.predict(vec_player[np.newaxis,:])[0]\n",
    "        #print('Normal',res_player)\n",
    "\n",
    "    else :\n",
    "        res_player_list = player_model.predict_proba(vec_player[np.newaxis,:])[0]\n",
    "        labels = list(player_model.classes_)\n",
    "        filtered_labels = [labels.index(str(i)) for i in filtered_indices]\n",
    "        #print(res_player_list.shape, res_player_list[filtered_labels])\n",
    "        id_filtered = np.argmax(res_player_list[filtered_labels])\n",
    "        res_player = filtered_indices[id_filtered]\n",
    "        #print('Filtered',res_player)\n",
    "    prediction.append(res_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 76% of 15 min chunk don't provide any information the game. In this case we get around **12.5%** accuracy. Which is not that bad btw, with regard to the 230 players.\n",
    "- It means essentially that 24% of exemples might reveal the exact team ! In this case, the accuracy score go up to **51%** ! \n",
    "\n",
    "Overall, adding this knowledge brings up the accuracy from 12% to 21% ! Pretty good then !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694872046279047"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(validation_team=='None').sum()/validation_team.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)\n",
    "true_labels = [str(i) for i in validation_labels.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21688400823610157"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction,true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage of unknown team :  76.95 %\n",
      "Accuracy Score without knowledge on the team :  12.76 %\n",
      "Accuracy Score WITH knowledge on the team :  51.48 %\n"
     ]
    }
   ],
   "source": [
    "indices = (validation_team=='None')\n",
    "prediction_filter = []\n",
    "true_filter = []\n",
    "for i,val in enumerate(indices):\n",
    "    if val == False : \n",
    "        prediction_filter.append(str(prediction[i]))\n",
    "        true_filter.append(true_labels[i])\n",
    "        \n",
    "prediction_normal = []\n",
    "true_normal = []\n",
    "for i,val in enumerate(indices):\n",
    "    if val == True : \n",
    "        prediction_normal.append(str(prediction[i]))\n",
    "        true_normal.append(true_labels[i])\n",
    "print('Pourcentage of unknown team : ', np.round(100*(validation_team=='None').sum()/validation_team.shape[0],2),'%')\n",
    "print('Accuracy Score without knowledge on the team : ',np.round(accuracy_score(prediction_normal,true_normal)*100,2),'%')\n",
    "print('Accuracy Score WITH knowledge on the team : ', np.round(accuracy_score(prediction_filter,true_filter)*100,2),'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlclass",
   "language": "python",
   "name": "dlclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
